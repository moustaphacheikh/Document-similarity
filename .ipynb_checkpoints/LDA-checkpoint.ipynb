{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import itertools\n",
    "import gensim\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "alt_1_dirpath_train    = './data/train/1'\n",
    "alt_2_dirpath_train    = './data/train/2'\n",
    "alt_3_dirpath_train    = './data/train/3'\n",
    "alt_4_dirpath_train    = './data/train/4'\n",
    "alt_5_dirpath_train    = './data/train/5'\n",
    "alt_6_dirpath_train    = './data/train/6'\n",
    "alt_7_dirpath_train    = './data/train/7'\n",
    "alt_8_dirpath_train    = './data/train/8'\n",
    "alt_9_dirpath_train    = './data/train/9'\n",
    "alt_10_dirpath_train   = './data/train/10'\n",
    "dir_paths_train = [\n",
    "alt_1_dirpath_train, \n",
    "alt_2_dirpath_train ,\n",
    "alt_3_dirpath_train ,\n",
    "alt_4_dirpath_train ,\n",
    "alt_5_dirpath_train ,\n",
    "alt_6_dirpath_train ,\n",
    "alt_7_dirpath_train ,\n",
    "alt_8_dirpath_train ,\n",
    "alt_9_dirpath_train ,\n",
    "alt_10_dirpath_train\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "STOPWORDS =['vide']\n",
    "def remove_extra_whitespace(string):\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    string = re.sub(r'[^\\u0600-\\u06FF]', ' ', string)\n",
    "    return re.sub(r\"\\s{2,}\", \" \", string).strip()\n",
    "def process_message(message): \n",
    "    \n",
    "    \"\"\"\n",
    "    purpose: Given an input message, the function returns a processed version \n",
    "             of the message.\n",
    "    input: \n",
    "    message: A string\n",
    "    \n",
    "    output:\n",
    "    A list of tokens(words) with the following processing done on wach token:\n",
    "           1) The top and bottom blocks are removed \n",
    "           2) Lower the case of the doc\n",
    "           3) Tokenize the doc\n",
    "           4) Remove stopwords\n",
    "           5) Stem the words\n",
    "    \"\"\"\n",
    "    \n",
    "    # skip email headers (first block) and footer (last block)\n",
    "    #blocks = message.split('\\n')\n",
    "    #content = '\\n\\n'.join(blocks[1:-1])\n",
    "    content =remove_extra_whitespace(message)\n",
    "    return [t_ for t_ in content.split()]\n",
    "    #return [p_stemmer.stem(t_) for t_ in tokenize(content.lower()) if not t_ in STOPWORDS]\n",
    "\n",
    "\n",
    "def iter_Docs(dir_path):\n",
    "    print(\"Here in file: \",dir_path)\n",
    "    \n",
    "    \"\"\"\n",
    "    purpose: given a directory, this generator:\n",
    "             1) finds a valid file in the directory\n",
    "             2) extracts the document in the file,\n",
    "             3) calls process_message to process the document content\n",
    "             4) yields the processed document as list of tokens as returned by process_message\n",
    "    input:\n",
    "    dir_path: directory name to look for document files\n",
    "    \n",
    "    output:\n",
    "    List of tokens as returned by process_message for a document. \n",
    "    The output is yielded with each yield returning doc for a single file in dir_path. \n",
    "    \"\"\"\n",
    "    \n",
    "    for file_name in listdir(dir_path):\n",
    "        if isfile(join(dir_path, file_name)):\n",
    "            with open(join(dir_path, file_name), 'r',encoding='utf8') as f_:\n",
    "                yield process_message(f_.read())\n",
    "\n",
    "def get_num_of_topics():\n",
    "    return 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create data streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Chain the training/test data streams for each news group to create a single training/test stream \n",
    "def getTrainingDataStream():\n",
    "    stream_train = itertools.chain( iter_Docs(alt_1_dirpath_train), \n",
    "                                    iter_Docs(alt_2_dirpath_train), \n",
    "                                    iter_Docs(alt_3_dirpath_train), \n",
    "                                    iter_Docs(alt_4_dirpath_train), \n",
    "                                    iter_Docs(alt_5_dirpath_train), \n",
    "                                    iter_Docs(alt_6_dirpath_train), \n",
    "                                    iter_Docs(alt_7_dirpath_train), \n",
    "                                    iter_Docs(alt_8_dirpath_train), \n",
    "                                    iter_Docs(alt_9_dirpath_train), \n",
    "                                    iter_Docs(alt_10_dirpath_train))\n",
    "    return stream_train\n",
    "\n",
    "def getTestDataStream():\n",
    "    stream_test = itertools.chain(  iter_Docs(alt_1_dirpath_test), \n",
    "                                    iter_Docs(alt_2_dirpath_test), \n",
    "                                    iter_Docs(alt_3_dirpath_test), \n",
    "                                    iter_Docs(alt_4_dirpath_test), \n",
    "                                    iter_Docs(alt_5_dirpath_test), \n",
    "                                    iter_Docs(alt_6_dirpath_test), \n",
    "                                    iter_Docs(alt_7_dirpath_test), \n",
    "                                    iter_Docs(alt_8_dirpath_test), \n",
    "                                    iter_Docs(alt_9_dirpath_test), \n",
    "                                    iter_Docs(alt_10_dirpath_test))\n",
    "    return stream_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create vocab dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here in file:  ./data/train/1\n",
      "Here in file:  ./data/train/2\n",
      "Here in file:  ./data/train/3\n",
      "Here in file:  ./data/train/4\n",
      "Here in file:  ./data/train/5\n",
      "Here in file:  ./data/train/6\n",
      "Here in file:  ./data/train/7\n",
      "Here in file:  ./data/train/8\n",
      "Here in file:  ./data/train/9\n",
      "Here in file:  ./data/train/10\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "id2word_Docs: Dictionary of the entire training vocabulary\n",
    "                    with key being an id to get the word in the vocabulary.\n",
    "                    The dictionary also has the statistics of the words in the vacabulary. \n",
    "\"\"\"\n",
    "\n",
    "id2word_Docs = gensim.corpora.dictionary.Dictionary(getTrainingDataStream())\n",
    "# Remove all words which appear in less than 10 documents and in more than 10% of the documents.\n",
    "id2word_Docs.filter_extremes(no_below=10, no_above=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# A class for getting bag of words per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class NewsDocsCorpus(object):\n",
    "    def __init__(self, dir_path_list, dictionary, num_docs=None):\n",
    "        self.dir_path_list = dir_path_list\n",
    "        self.dictionary = dictionary\n",
    "        self.num_docs = num_docs\n",
    "    def __iter__(self):\n",
    "        for dir_path in self.dir_path_list:\n",
    "            for tokens in itertools.islice(iter_Docs(dir_path), self.num_docs):\n",
    "                yield self.dictionary.doc2bow(tokens)\n",
    "    def __len__(self):\n",
    "        return self.num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a generator which will yield a bag of words \n",
    "for each document in all the files belonging to every directory in dir_paths_train.\n",
    "\"\"\"\n",
    "corpus_train = NewsDocsCorpus(dir_paths_train, id2word_Docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here in file:  ./data/train/1\n",
      "Here in file:  ./data/train/2\n",
      "Here in file:  ./data/train/3\n",
      "Here in file:  ./data/train/4\n",
      "Here in file:  ./data/train/5\n",
      "Here in file:  ./data/train/6\n",
      "Here in file:  ./data/train/7\n",
      "Here in file:  ./data/train/8\n",
      "Here in file:  ./data/train/9\n",
      "Here in file:  ./data/train/10\n",
      "Here in file:  ./data/train/1\n",
      "Here in file:  ./data/train/2\n",
      "Here in file:  ./data/train/3\n",
      "Here in file:  ./data/train/4\n",
      "Here in file:  ./data/train/5\n",
      "Here in file:  ./data/train/6\n",
      "Here in file:  ./data/train/7\n",
      "Here in file:  ./data/train/8\n",
      "Here in file:  ./data/train/9\n",
      "Here in file:  ./data/train/10\n",
      "Here in file:  ./data/train/1\n",
      "Here in file:  ./data/train/2\n",
      "Here in file:  ./data/train/3\n",
      "Here in file:  ./data/train/4\n",
      "Here in file:  ./data/train/5\n",
      "Here in file:  ./data/train/6\n",
      "Here in file:  ./data/train/7\n",
      "Here in file:  ./data/train/8\n",
      "Here in file:  ./data/train/9\n",
      "Here in file:  ./data/train/10\n",
      "Here in file:  ./data/train/1\n",
      "Here in file:  ./data/train/2\n",
      "Here in file:  ./data/train/3\n",
      "Here in file:  ./data/train/4\n",
      "Here in file:  ./data/train/5\n",
      "Here in file:  ./data/train/6\n",
      "Here in file:  ./data/train/7\n",
      "Here in file:  ./data/train/8\n",
      "Here in file:  ./data/train/9\n",
      "Here in file:  ./data/train/10\n",
      "Here in file:  ./data/train/1\n",
      "Here in file:  ./data/train/2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the LDA model.\n",
    "The number of topics is chosen to be three as we want the model to be able to catagorize three distinct news groups.\n",
    "\"\"\"\n",
    "lda_model =gensim.models.ldamulticore.LdaMulticore(corpus_train, \n",
    "                                   num_topics=get_num_of_topics(), \n",
    "                                   id2word=id2word_Docs, \n",
    "                                   passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Display all the topics and the 5 most relevant words in each topic\n",
    "\"\"\"\n",
    "lda_model.show_topics(-1, num_words=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "alt_1_dirpath_test    = './data/test/1'\n",
    "alt_2_dirpath_test    = './data/test/2'\n",
    "alt_3_dirpath_test    = './data/test/3'\n",
    "alt_4_dirpath_test    = './data/test/4'\n",
    "alt_5_dirpath_test    = './data/test/5'\n",
    "alt_6_dirpath_test    = './data/test/6'\n",
    "alt_7_dirpath_test    = './data/test/7'\n",
    "alt_8_dirpath_test    = './data/test/8'\n",
    "alt_9_dirpath_test    = './data/test/9'\n",
    "alt_10_dirpath_test   = './data/test/10'\n",
    "dir_paths_test = [\n",
    "alt_1_dirpath_test, \n",
    "alt_2_dirpath_test ,\n",
    "alt_3_dirpath_test ,\n",
    "alt_4_dirpath_test ,\n",
    "alt_5_dirpath_test ,\n",
    "alt_6_dirpath_test ,\n",
    "alt_7_dirpath_test ,\n",
    "alt_8_dirpath_test ,\n",
    "alt_9_dirpath_test ,\n",
    "alt_10_dirpath_test\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Utility function to get topic-document probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_topic_probabilities(dir_path, num_docs=None):\n",
    "    \"\"\"\n",
    "    purpose: for num_docs documents in dir_path, return the topic-document probabilities \n",
    "                    T1    T2    T3                        \n",
    "             doc1 [[p_11, p_12, p_13],\n",
    "             doc2  [p_21, p_22, p_23], \n",
    "                    ....\n",
    "                    ....\n",
    "         num_docs  [p_num_docsT1, p_num_docsT2, p_num_docsT3]]\n",
    "    input:\n",
    "       dir_path: directory path for the files whose topic-document probability we want to find \n",
    "       num_docs: number of documents whose topic-document probability we want to find\n",
    "    output:\n",
    "       return the topic-document probabilities\n",
    "    \"\"\"\n",
    "    probability = []\n",
    "    for c_ in itertools.islice(NewsDocsCorpus([dir_path], id2word_Docs, num_docs), num_docs):\n",
    "        current_prob = [0] * get_num_of_topics()\n",
    "        for topic, prob in lda_model.get_document_topics(c_):\n",
    "            current_prob[topic] = prob  \n",
    "        probability.append(current_prob)\n",
    "        #print(probability)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get the topic-document probabilities\n",
    "alt_1_prob_test = get_topic_probabilities(alt_1_dirpath_test)\n",
    "alt_2_prob_test = get_topic_probabilities(alt_2_dirpath_test)\n",
    "alt_3_prob_test = get_topic_probabilities(alt_3_dirpath_test)\n",
    "alt_4_prob_test = get_topic_probabilities(alt_4_dirpath_test)\n",
    "alt_5_prob_test = get_topic_probabilities(alt_5_dirpath_test)\n",
    "alt_6_prob_test = get_topic_probabilities(alt_6_dirpath_test)\n",
    "alt_7_prob_test = get_topic_probabilities(alt_7_dirpath_test)\n",
    "alt_8_prob_test = get_topic_probabilities(alt_8_dirpath_test)\n",
    "alt_9_prob_test = get_topic_probabilities(alt_9_dirpath_test)\n",
    "alt_10_prob_test= get_topic_probabilities(alt_10_dirpath_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Plot the topic-document probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_probabilities(probabilities, col='red', topics=(0,1), legend=''):\n",
    "    plt.scatter([p_[0] for p_ in probabilities], [p_[1] for p_ in probabilities], color=col,  label=legend)\n",
    "import pylab \n",
    "import matplotlib.cm as cm\n",
    "colors = cm.rainbow(np.linspace(0, 1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_probabilities(alt_1_prob_test, col=colors[0], legend='topic 1')\n",
    "plot_probabilities(alt_2_prob_test, col=colors[1], legend='topic 2')\n",
    "plot_probabilities(alt_3_prob_test, col=colors[2], legend='topic 3')\n",
    "plot_probabilities(alt_4_prob_test, col=colors[3], legend='topic 4')\n",
    "plot_probabilities(alt_5_prob_test, col=colors[4], legend='topic 5')\n",
    "plot_probabilities(alt_6_prob_test, col=colors[5], legend='topic 6')\n",
    "plot_probabilities(alt_7_prob_test, col=colors[6], legend='topic 7')\n",
    "plot_probabilities(alt_8_prob_test, col=colors[7], legend='topic 8')\n",
    "plot_probabilities(alt_9_prob_test, col=colors[8], legend='topic 9')\n",
    "plot_probabilities(alt_10_prob_test, col=colors[9], legend='topic 10')\n",
    "pylab.ylim([-0.05,1.05])\n",
    "pylab.xlim([-0.05,1.05])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
